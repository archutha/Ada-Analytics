{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_4111_oW7Ck",
        "outputId": "c709a06d-61cb-487d-fe6c-d1534af758ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install praw pandas requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_tickers():\n",
        "    nasdaq = pd.read_csv(\"nasdaq_.csv\")\n",
        "    nyse = pd.read_csv(\"nyse.csv\")\n",
        "\n",
        "    # Make sure the column name is correct; use 'Symbol' or 'ACT Symbol'\n",
        "    if 'Symbol' in nasdaq.columns:\n",
        "        nasdaq_tickers = set(nasdaq['Symbol'].str.upper())\n",
        "    else:\n",
        "        nasdaq_tickers = set(nasdaq['ACT Symbol'].str.upper())\n",
        "\n",
        "    if 'Symbol' in nyse.columns:\n",
        "        nyse_tickers = set(nyse['Symbol'].str.upper())\n",
        "    else:\n",
        "        nyse_tickers = set(nyse['ACT Symbol'].str.upper())\n",
        "\n",
        "    all_tickers = nasdaq_tickers.union(nyse_tickers)\n",
        "    return all_tickers\n",
        "\n",
        "# Load and print count\n",
        "valid_tickers = load_tickers()\n",
        "print(f\"Loaded {len(valid_tickers)} valid tickers.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDqgR6l1XC7b",
        "outputId": "61454f45-1a31-49da-da41-75a0118069c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 7709 valid tickers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"Lk7amzjYlHw4NZN4jVeMOA\",\n",
        "    client_secret=\"4Xv7mFZcXHbcvUkQG98tMau-BvEFFg\",\n",
        "    user_agent=\"WSB_Sentiment_Bot/1.0 by Defiant-Fee-533\",\n",
        "    check_for_async=False\n",
        ")\n",
        "\n",
        "subreddits = ['wallstreetbets', 'stocks', 'investing']\n",
        "post_limit = 200  # number of posts to scan per subreddit\n",
        "\n",
        "# Regex to extract uppercase words 1-5 letters (possible ticker symbols)\n",
        "ticker_candidate_pattern = re.compile(r'\\b[A-Z]{1,5}\\b')\n",
        "\n",
        "def find_trending_tickers():\n",
        "    all_candidates = []\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        print(f\"Scanning r/{subreddit_name}...\")\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.hot(limit=post_limit):\n",
        "            text = post.title.upper()\n",
        "            candidates = ticker_candidate_pattern.findall(text)\n",
        "            all_candidates.extend(candidates)\n",
        "            time.sleep(0.1)  # polite pause to avoid rate limit\n",
        "\n",
        "    # Filter candidates by whether they are valid tickers\n",
        "    filtered = [t for t in all_candidates if t in valid_tickers]\n",
        "\n",
        "    # Count frequencies and get top 20 trending tickers\n",
        "    counter = Counter(filtered)\n",
        "    top_20 = counter.most_common(20)\n",
        "\n",
        "    print(\"\\nTop 20 trending tickers detected on Reddit:\")\n",
        "    for ticker, count in top_20:\n",
        "        print(f\"{ticker}: {count} mentions\")\n",
        "\n",
        "    return [t[0] for t in top_20]\n"
      ],
      "metadata": {
        "id": "jLm0CWGsbOVu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_posts_for_ticker(ticker, max_posts=15):\n",
        "    posts = []\n",
        "    query = f'title:{ticker} OR selftext:{ticker}'\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.search(query, sort='new', limit=max_posts):\n",
        "            posts.append({\n",
        "                'ticker': ticker,\n",
        "                'post_id': post.id,\n",
        "                'title': post.title,\n",
        "                'selftext': post.selftext,\n",
        "                'url': post.url,\n",
        "                'created_utc': post.created_utc,\n",
        "                'score': post.score,\n",
        "                'num_comments': post.num_comments,\n",
        "                'subreddit': subreddit_name\n",
        "            })\n",
        "            if len(posts) >= max_posts:\n",
        "                break\n",
        "        if len(posts) >= max_posts:\n",
        "            break\n",
        "    return posts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    valid_tickers = load_tickers()\n",
        "    trending_tickers = find_trending_tickers()\n",
        "\n",
        "    all_posts = []\n",
        "    for ticker in trending_tickers:\n",
        "        print(f\"\\nFetching posts for {ticker}...\")\n",
        "        posts = get_posts_for_ticker(ticker)\n",
        "        all_posts.extend(posts)\n",
        "        print(f\"Fetched {len(posts)} posts for {ticker}\")\n",
        "        time.sleep(1)\n",
        "    # Convert the list of posts to a DataFrame\n",
        "    df = pd.DataFrame(all_posts)\n",
        "    # Save the DataFrame to a CSV file\n",
        "    output_file = \"reddit_trending_tickers_posts.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nSaved extracted data to '{output_file}'\")\n"
      ],
      "metadata": {
        "id": "isR7OJZqXHQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761d9329-7240-49ba-b321-9bec022c9817"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning r/wallstreetbets...\n",
            "Scanning r/stocks...\n",
            "Scanning r/investing...\n",
            "\n",
            "Top 20 trending tickers detected on Reddit:\n",
            "FOR: 76 mentions\n",
            "A: 75 mentions\n",
            "S: 74 mentions\n",
            "ON: 62 mentions\n",
            "UNH: 47 mentions\n",
            "YOU: 41 mentions\n",
            "OR: 39 mentions\n",
            "IT: 37 mentions\n",
            "ARE: 36 mentions\n",
            "CAN: 19 mentions\n",
            "UP: 18 mentions\n",
            "T: 17 mentions\n",
            "BE: 16 mentions\n",
            "NOW: 15 mentions\n",
            "ALL: 13 mentions\n",
            "AM: 13 mentions\n",
            "AI: 13 mentions\n",
            "VS: 13 mentions\n",
            "U: 13 mentions\n",
            "M: 12 mentions\n",
            "\n",
            "Fetching posts for FOR...\n",
            "Fetched 15 posts for FOR\n",
            "\n",
            "Fetching posts for A...\n",
            "Fetched 15 posts for A\n",
            "\n",
            "Fetching posts for S...\n",
            "Fetched 15 posts for S\n",
            "\n",
            "Fetching posts for ON...\n",
            "Fetched 15 posts for ON\n",
            "\n",
            "Fetching posts for UNH...\n",
            "Fetched 15 posts for UNH\n",
            "\n",
            "Fetching posts for YOU...\n",
            "Fetched 15 posts for YOU\n",
            "\n",
            "Fetching posts for OR...\n",
            "Fetched 15 posts for OR\n",
            "\n",
            "Fetching posts for IT...\n",
            "Fetched 15 posts for IT\n",
            "\n",
            "Fetching posts for ARE...\n",
            "Fetched 15 posts for ARE\n",
            "\n",
            "Fetching posts for CAN...\n",
            "Fetched 15 posts for CAN\n",
            "\n",
            "Fetching posts for UP...\n",
            "Fetched 15 posts for UP\n",
            "\n",
            "Fetching posts for T...\n",
            "Fetched 15 posts for T\n",
            "\n",
            "Fetching posts for BE...\n",
            "Fetched 15 posts for BE\n",
            "\n",
            "Fetching posts for NOW...\n",
            "Fetched 15 posts for NOW\n",
            "\n",
            "Fetching posts for ALL...\n",
            "Fetched 15 posts for ALL\n",
            "\n",
            "Fetching posts for AM...\n",
            "Fetched 15 posts for AM\n",
            "\n",
            "Fetching posts for AI...\n",
            "Fetched 15 posts for AI\n",
            "\n",
            "Fetching posts for VS...\n",
            "Fetched 15 posts for VS\n",
            "\n",
            "Fetching posts for U...\n",
            "Fetched 15 posts for U\n",
            "\n",
            "Fetching posts for M...\n",
            "Fetched 15 posts for M\n",
            "\n",
            "Saved extracted data to 'reddit_trending_tickers_posts.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEEUc6n9d4ZI",
        "outputId": "f4d8f752-75ae-4f01-92ea-c00705886757"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.61)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.4)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "def get_sp500_tickers():\n",
        "    # Fetch S&P 500 tickers\n",
        "    sp500 = yf.Ticker(\"^GSPC\")\n",
        "\n",
        "    # yfinance does not have a direct method for tickers list,\n",
        "    # but there's a popular workaround to get the list via Wikipedia\n",
        "    sp500_table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "    sp500_df = sp500_table[0]\n",
        "\n",
        "    # Get the Symbol column as a list\n",
        "    tickers = sp500_df['Symbol'].tolist()\n",
        "    # Make uppercase for consistency\n",
        "    tickers = [t.upper() for t in tickers]\n",
        "\n",
        "    return tickers\n",
        "\n",
        "# Example usage\n",
        "valid_tickers = get_sp500_tickers()\n",
        "print(valid_tickers[:10])  # print first 10 tickers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYu6X4xhmCRP",
        "outputId": "e3cd22f7-096c-485d-959c-1d84e3085024"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_tickers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pNTTFiezmHrS",
        "outputId": "8d0d1989-fb8a-4b2c-a39c-a0641d487a42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MMM',\n",
              " 'AOS',\n",
              " 'ABT',\n",
              " 'ABBV',\n",
              " 'ACN',\n",
              " 'ADBE',\n",
              " 'AMD',\n",
              " 'AES',\n",
              " 'AFL',\n",
              " 'A',\n",
              " 'APD',\n",
              " 'ABNB',\n",
              " 'AKAM',\n",
              " 'ALB',\n",
              " 'ARE',\n",
              " 'ALGN',\n",
              " 'ALLE',\n",
              " 'LNT',\n",
              " 'ALL',\n",
              " 'GOOGL',\n",
              " 'GOOG',\n",
              " 'MO',\n",
              " 'AMZN',\n",
              " 'AMCR',\n",
              " 'AEE',\n",
              " 'AEP',\n",
              " 'AXP',\n",
              " 'AIG',\n",
              " 'AMT',\n",
              " 'AWK',\n",
              " 'AMP',\n",
              " 'AME',\n",
              " 'AMGN',\n",
              " 'APH',\n",
              " 'ADI',\n",
              " 'ANSS',\n",
              " 'AON',\n",
              " 'APA',\n",
              " 'APO',\n",
              " 'AAPL',\n",
              " 'AMAT',\n",
              " 'APTV',\n",
              " 'ACGL',\n",
              " 'ADM',\n",
              " 'ANET',\n",
              " 'AJG',\n",
              " 'AIZ',\n",
              " 'T',\n",
              " 'ATO',\n",
              " 'ADSK',\n",
              " 'ADP',\n",
              " 'AZO',\n",
              " 'AVB',\n",
              " 'AVY',\n",
              " 'AXON',\n",
              " 'BKR',\n",
              " 'BALL',\n",
              " 'BAC',\n",
              " 'BAX',\n",
              " 'BDX',\n",
              " 'BRK.B',\n",
              " 'BBY',\n",
              " 'TECH',\n",
              " 'BIIB',\n",
              " 'BLK',\n",
              " 'BX',\n",
              " 'BK',\n",
              " 'BA',\n",
              " 'BKNG',\n",
              " 'BSX',\n",
              " 'BMY',\n",
              " 'AVGO',\n",
              " 'BR',\n",
              " 'BRO',\n",
              " 'BF.B',\n",
              " 'BLDR',\n",
              " 'BG',\n",
              " 'BXP',\n",
              " 'CHRW',\n",
              " 'CDNS',\n",
              " 'CZR',\n",
              " 'CPT',\n",
              " 'CPB',\n",
              " 'COF',\n",
              " 'CAH',\n",
              " 'KMX',\n",
              " 'CCL',\n",
              " 'CARR',\n",
              " 'CAT',\n",
              " 'CBOE',\n",
              " 'CBRE',\n",
              " 'CDW',\n",
              " 'COR',\n",
              " 'CNC',\n",
              " 'CNP',\n",
              " 'CF',\n",
              " 'CRL',\n",
              " 'SCHW',\n",
              " 'CHTR',\n",
              " 'CVX',\n",
              " 'CMG',\n",
              " 'CB',\n",
              " 'CHD',\n",
              " 'CI',\n",
              " 'CINF',\n",
              " 'CTAS',\n",
              " 'CSCO',\n",
              " 'C',\n",
              " 'CFG',\n",
              " 'CLX',\n",
              " 'CME',\n",
              " 'CMS',\n",
              " 'KO',\n",
              " 'CTSH',\n",
              " 'COIN',\n",
              " 'CL',\n",
              " 'CMCSA',\n",
              " 'CAG',\n",
              " 'COP',\n",
              " 'ED',\n",
              " 'STZ',\n",
              " 'CEG',\n",
              " 'COO',\n",
              " 'CPRT',\n",
              " 'GLW',\n",
              " 'CPAY',\n",
              " 'CTVA',\n",
              " 'CSGP',\n",
              " 'COST',\n",
              " 'CTRA',\n",
              " 'CRWD',\n",
              " 'CCI',\n",
              " 'CSX',\n",
              " 'CMI',\n",
              " 'CVS',\n",
              " 'DHR',\n",
              " 'DRI',\n",
              " 'DVA',\n",
              " 'DAY',\n",
              " 'DECK',\n",
              " 'DE',\n",
              " 'DELL',\n",
              " 'DAL',\n",
              " 'DVN',\n",
              " 'DXCM',\n",
              " 'FANG',\n",
              " 'DLR',\n",
              " 'DG',\n",
              " 'DLTR',\n",
              " 'D',\n",
              " 'DPZ',\n",
              " 'DASH',\n",
              " 'DOV',\n",
              " 'DOW',\n",
              " 'DHI',\n",
              " 'DTE',\n",
              " 'DUK',\n",
              " 'DD',\n",
              " 'EMN',\n",
              " 'ETN',\n",
              " 'EBAY',\n",
              " 'ECL',\n",
              " 'EIX',\n",
              " 'EW',\n",
              " 'EA',\n",
              " 'ELV',\n",
              " 'EMR',\n",
              " 'ENPH',\n",
              " 'ETR',\n",
              " 'EOG',\n",
              " 'EPAM',\n",
              " 'EQT',\n",
              " 'EFX',\n",
              " 'EQIX',\n",
              " 'EQR',\n",
              " 'ERIE',\n",
              " 'ESS',\n",
              " 'EL',\n",
              " 'EG',\n",
              " 'EVRG',\n",
              " 'ES',\n",
              " 'EXC',\n",
              " 'EXE',\n",
              " 'EXPE',\n",
              " 'EXPD',\n",
              " 'EXR',\n",
              " 'XOM',\n",
              " 'FFIV',\n",
              " 'FDS',\n",
              " 'FICO',\n",
              " 'FAST',\n",
              " 'FRT',\n",
              " 'FDX',\n",
              " 'FIS',\n",
              " 'FITB',\n",
              " 'FSLR',\n",
              " 'FE',\n",
              " 'FI',\n",
              " 'F',\n",
              " 'FTNT',\n",
              " 'FTV',\n",
              " 'FOXA',\n",
              " 'FOX',\n",
              " 'BEN',\n",
              " 'FCX',\n",
              " 'GRMN',\n",
              " 'IT',\n",
              " 'GE',\n",
              " 'GEHC',\n",
              " 'GEV',\n",
              " 'GEN',\n",
              " 'GNRC',\n",
              " 'GD',\n",
              " 'GIS',\n",
              " 'GM',\n",
              " 'GPC',\n",
              " 'GILD',\n",
              " 'GPN',\n",
              " 'GL',\n",
              " 'GDDY',\n",
              " 'GS',\n",
              " 'HAL',\n",
              " 'HIG',\n",
              " 'HAS',\n",
              " 'HCA',\n",
              " 'DOC',\n",
              " 'HSIC',\n",
              " 'HSY',\n",
              " 'HES',\n",
              " 'HPE',\n",
              " 'HLT',\n",
              " 'HOLX',\n",
              " 'HD',\n",
              " 'HON',\n",
              " 'HRL',\n",
              " 'HST',\n",
              " 'HWM',\n",
              " 'HPQ',\n",
              " 'HUBB',\n",
              " 'HUM',\n",
              " 'HBAN',\n",
              " 'HII',\n",
              " 'IBM',\n",
              " 'IEX',\n",
              " 'IDXX',\n",
              " 'ITW',\n",
              " 'INCY',\n",
              " 'IR',\n",
              " 'PODD',\n",
              " 'INTC',\n",
              " 'ICE',\n",
              " 'IFF',\n",
              " 'IP',\n",
              " 'IPG',\n",
              " 'INTU',\n",
              " 'ISRG',\n",
              " 'IVZ',\n",
              " 'INVH',\n",
              " 'IQV',\n",
              " 'IRM',\n",
              " 'JBHT',\n",
              " 'JBL',\n",
              " 'JKHY',\n",
              " 'J',\n",
              " 'JNJ',\n",
              " 'JCI',\n",
              " 'JPM',\n",
              " 'JNPR',\n",
              " 'K',\n",
              " 'KVUE',\n",
              " 'KDP',\n",
              " 'KEY',\n",
              " 'KEYS',\n",
              " 'KMB',\n",
              " 'KIM',\n",
              " 'KMI',\n",
              " 'KKR',\n",
              " 'KLAC',\n",
              " 'KHC',\n",
              " 'KR',\n",
              " 'LHX',\n",
              " 'LH',\n",
              " 'LRCX',\n",
              " 'LW',\n",
              " 'LVS',\n",
              " 'LDOS',\n",
              " 'LEN',\n",
              " 'LII',\n",
              " 'LLY',\n",
              " 'LIN',\n",
              " 'LYV',\n",
              " 'LKQ',\n",
              " 'LMT',\n",
              " 'L',\n",
              " 'LOW',\n",
              " 'LULU',\n",
              " 'LYB',\n",
              " 'MTB',\n",
              " 'MPC',\n",
              " 'MKTX',\n",
              " 'MAR',\n",
              " 'MMC',\n",
              " 'MLM',\n",
              " 'MAS',\n",
              " 'MA',\n",
              " 'MTCH',\n",
              " 'MKC',\n",
              " 'MCD',\n",
              " 'MCK',\n",
              " 'MDT',\n",
              " 'MRK',\n",
              " 'META',\n",
              " 'MET',\n",
              " 'MTD',\n",
              " 'MGM',\n",
              " 'MCHP',\n",
              " 'MU',\n",
              " 'MSFT',\n",
              " 'MAA',\n",
              " 'MRNA',\n",
              " 'MHK',\n",
              " 'MOH',\n",
              " 'TAP',\n",
              " 'MDLZ',\n",
              " 'MPWR',\n",
              " 'MNST',\n",
              " 'MCO',\n",
              " 'MS',\n",
              " 'MOS',\n",
              " 'MSI',\n",
              " 'MSCI',\n",
              " 'NDAQ',\n",
              " 'NTAP',\n",
              " 'NFLX',\n",
              " 'NEM',\n",
              " 'NWSA',\n",
              " 'NWS',\n",
              " 'NEE',\n",
              " 'NKE',\n",
              " 'NI',\n",
              " 'NDSN',\n",
              " 'NSC',\n",
              " 'NTRS',\n",
              " 'NOC',\n",
              " 'NCLH',\n",
              " 'NRG',\n",
              " 'NUE',\n",
              " 'NVDA',\n",
              " 'NVR',\n",
              " 'NXPI',\n",
              " 'ORLY',\n",
              " 'OXY',\n",
              " 'ODFL',\n",
              " 'OMC',\n",
              " 'ON',\n",
              " 'OKE',\n",
              " 'ORCL',\n",
              " 'OTIS',\n",
              " 'PCAR',\n",
              " 'PKG',\n",
              " 'PLTR',\n",
              " 'PANW',\n",
              " 'PARA',\n",
              " 'PH',\n",
              " 'PAYX',\n",
              " 'PAYC',\n",
              " 'PYPL',\n",
              " 'PNR',\n",
              " 'PEP',\n",
              " 'PFE',\n",
              " 'PCG',\n",
              " 'PM',\n",
              " 'PSX',\n",
              " 'PNW',\n",
              " 'PNC',\n",
              " 'POOL',\n",
              " 'PPG',\n",
              " 'PPL',\n",
              " 'PFG',\n",
              " 'PG',\n",
              " 'PGR',\n",
              " 'PLD',\n",
              " 'PRU',\n",
              " 'PEG',\n",
              " 'PTC',\n",
              " 'PSA',\n",
              " 'PHM',\n",
              " 'PWR',\n",
              " 'QCOM',\n",
              " 'DGX',\n",
              " 'RL',\n",
              " 'RJF',\n",
              " 'RTX',\n",
              " 'O',\n",
              " 'REG',\n",
              " 'REGN',\n",
              " 'RF',\n",
              " 'RSG',\n",
              " 'RMD',\n",
              " 'RVTY',\n",
              " 'ROK',\n",
              " 'ROL',\n",
              " 'ROP',\n",
              " 'ROST',\n",
              " 'RCL',\n",
              " 'SPGI',\n",
              " 'CRM',\n",
              " 'SBAC',\n",
              " 'SLB',\n",
              " 'STX',\n",
              " 'SRE',\n",
              " 'NOW',\n",
              " 'SHW',\n",
              " 'SPG',\n",
              " 'SWKS',\n",
              " 'SJM',\n",
              " 'SW',\n",
              " 'SNA',\n",
              " 'SOLV',\n",
              " 'SO',\n",
              " 'LUV',\n",
              " 'SWK',\n",
              " 'SBUX',\n",
              " 'STT',\n",
              " 'STLD',\n",
              " 'STE',\n",
              " 'SYK',\n",
              " 'SMCI',\n",
              " 'SYF',\n",
              " 'SNPS',\n",
              " 'SYY',\n",
              " 'TMUS',\n",
              " 'TROW',\n",
              " 'TTWO',\n",
              " 'TPR',\n",
              " 'TRGP',\n",
              " 'TGT',\n",
              " 'TEL',\n",
              " 'TDY',\n",
              " 'TER',\n",
              " 'TSLA',\n",
              " 'TXN',\n",
              " 'TPL',\n",
              " 'TXT',\n",
              " 'TMO',\n",
              " 'TJX',\n",
              " 'TKO',\n",
              " 'TSCO',\n",
              " 'TT',\n",
              " 'TDG',\n",
              " 'TRV',\n",
              " 'TRMB',\n",
              " 'TFC',\n",
              " 'TYL',\n",
              " 'TSN',\n",
              " 'USB',\n",
              " 'UBER',\n",
              " 'UDR',\n",
              " 'ULTA',\n",
              " 'UNP',\n",
              " 'UAL',\n",
              " 'UPS',\n",
              " 'URI',\n",
              " 'UNH',\n",
              " 'UHS',\n",
              " 'VLO',\n",
              " 'VTR',\n",
              " 'VLTO',\n",
              " 'VRSN',\n",
              " 'VRSK',\n",
              " 'VZ',\n",
              " 'VRTX',\n",
              " 'VTRS',\n",
              " 'VICI',\n",
              " 'V',\n",
              " 'VST',\n",
              " 'VMC',\n",
              " 'WRB',\n",
              " 'GWW',\n",
              " 'WAB',\n",
              " 'WBA',\n",
              " 'WMT',\n",
              " 'DIS',\n",
              " 'WBD',\n",
              " 'WM',\n",
              " 'WAT',\n",
              " 'WEC',\n",
              " 'WFC',\n",
              " 'WELL',\n",
              " 'WST',\n",
              " 'WDC',\n",
              " 'WY',\n",
              " 'WSM',\n",
              " 'WMB',\n",
              " 'WTW',\n",
              " 'WDAY',\n",
              " 'WYNN',\n",
              " 'XEL',\n",
              " 'XYL',\n",
              " 'YUM',\n",
              " 'ZBRA',\n",
              " 'ZBH',\n",
              " 'ZTS']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Reddit API setup\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"Lk7amzjYlHw4NZN4jVeMOA\",\n",
        "    client_secret=\"4Xv7mFZcXHbcvUkQG98tMau-BvEFFg\",\n",
        "    user_agent=\"WSB_Sentiment_Bot/1.0 by Defiant-Fee-533\",\n",
        "    check_for_async=False\n",
        ")\n",
        "\n",
        "subreddits = ['wallstreetbets', 'stocks', 'investing']\n",
        "post_limit = 200  # number of posts to scan per subreddit\n",
        "\n",
        "# Regex to extract uppercase ticker symbols, allowing optional dot (like BRK.B)\n",
        "ticker_candidate_pattern = re.compile(r'\\b[A-Z]{1,5}(?:\\.[A-Z])?\\b')\n",
        "\n",
        "\n",
        "def find_trending_tickers():\n",
        "    all_candidates = []\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        print(f\"Scanning r/{subreddit_name}...\")\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.hot(limit=post_limit):\n",
        "            text = post.title.upper()\n",
        "            candidates = ticker_candidate_pattern.findall(text)\n",
        "            all_candidates.extend(candidates)\n",
        "            time.sleep(0.1)  # polite pause to avoid rate limit\n",
        "\n",
        "    # Filter candidates by whether they are valid tickers\n",
        "    filtered = [t for t in all_candidates if t in valid_tickers]\n",
        "\n",
        "    # Count frequencies and get top 20 trending tickers\n",
        "    counter = Counter(filtered)\n",
        "    top_20 = counter.most_common(20)\n",
        "\n",
        "    print(\"\\nTop 20 trending tickers detected on Reddit:\")\n",
        "    for ticker, count in top_20:\n",
        "        print(f\"{ticker}: {count} mentions\")\n",
        "\n",
        "    return [t[0] for t in top_20]\n",
        "\n",
        "def get_posts_for_ticker(ticker, max_posts=15):\n",
        "    posts = []\n",
        "    query = f'title:{ticker} OR selftext:{ticker}'\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for post in subreddit.search(query, sort='new', limit=max_posts):\n",
        "            posts.append({\n",
        "                'ticker': ticker,\n",
        "                'post_id': post.id,\n",
        "                'title': post.title,\n",
        "                'selftext': post.selftext,\n",
        "                'url': post.url,\n",
        "                'created_utc': post.created_utc,\n",
        "                'score': post.score,\n",
        "                'num_comments': post.num_comments,\n",
        "                'subreddit': subreddit_name\n",
        "            })\n",
        "            if len(posts) >= max_posts:\n",
        "                break\n",
        "        if len(posts) >= max_posts:\n",
        "            break\n",
        "    return posts\n",
        "\n",
        "def save_posts_to_csv(posts, filename='reddit_stock_posts.csv'):\n",
        "    if not posts:\n",
        "        print(\"No posts to save.\")\n",
        "        return\n",
        "\n",
        "    keys = posts[0].keys()\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(posts)\n",
        "\n",
        "    print(f\"Saved {len(posts)} posts to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trending_tickers = find_trending_tickers()\n",
        "\n",
        "    all_posts = []\n",
        "    for ticker in trending_tickers:\n",
        "        print(f\"\\nFetching posts for {ticker}...\")\n",
        "        posts = get_posts_for_ticker(ticker)\n",
        "        all_posts.extend(posts)\n",
        "        print(f\"Fetched {len(posts)} posts for {ticker}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    save_posts_to_csv(all_posts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91fqxQLRmQbD",
        "outputId": "b998a6ad-92b7-470a-b928-44ac49fa722f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning r/wallstreetbets...\n",
            "Scanning r/stocks...\n",
            "Scanning r/investing...\n",
            "\n",
            "Top 20 trending tickers detected on Reddit:\n",
            "A: 79 mentions\n",
            "ON: 69 mentions\n",
            "IT: 38 mentions\n",
            "UNH: 37 mentions\n",
            "ARE: 36 mentions\n",
            "T: 20 mentions\n",
            "ALL: 15 mentions\n",
            "NOW: 14 mentions\n",
            "DAY: 8 mentions\n",
            "NVDA: 7 mentions\n",
            "SO: 7 mentions\n",
            "HAS: 7 mentions\n",
            "TECH: 4 mentions\n",
            "GOOGL: 3 mentions\n",
            "D: 3 mentions\n",
            "GOOG: 3 mentions\n",
            "AMD: 3 mentions\n",
            "PLTR: 2 mentions\n",
            "LOW: 2 mentions\n",
            "RTX: 2 mentions\n",
            "\n",
            "Fetching posts for A...\n",
            "Fetched 15 posts for A\n",
            "\n",
            "Fetching posts for ON...\n",
            "Fetched 15 posts for ON\n",
            "\n",
            "Fetching posts for IT...\n",
            "Fetched 15 posts for IT\n",
            "\n",
            "Fetching posts for UNH...\n",
            "Fetched 15 posts for UNH\n",
            "\n",
            "Fetching posts for ARE...\n",
            "Fetched 15 posts for ARE\n",
            "\n",
            "Fetching posts for T...\n",
            "Fetched 15 posts for T\n",
            "\n",
            "Fetching posts for ALL...\n",
            "Fetched 15 posts for ALL\n",
            "\n",
            "Fetching posts for NOW...\n",
            "Fetched 15 posts for NOW\n",
            "\n",
            "Fetching posts for DAY...\n",
            "Fetched 15 posts for DAY\n",
            "\n",
            "Fetching posts for NVDA...\n",
            "Fetched 15 posts for NVDA\n",
            "\n",
            "Fetching posts for SO...\n",
            "Fetched 15 posts for SO\n",
            "\n",
            "Fetching posts for HAS...\n",
            "Fetched 15 posts for HAS\n",
            "\n",
            "Fetching posts for TECH...\n",
            "Fetched 15 posts for TECH\n",
            "\n",
            "Fetching posts for GOOGL...\n",
            "Fetched 15 posts for GOOGL\n",
            "\n",
            "Fetching posts for D...\n",
            "Fetched 15 posts for D\n",
            "\n",
            "Fetching posts for GOOG...\n",
            "Fetched 15 posts for GOOG\n",
            "\n",
            "Fetching posts for AMD...\n",
            "Fetched 15 posts for AMD\n",
            "\n",
            "Fetching posts for PLTR...\n",
            "Fetched 15 posts for PLTR\n",
            "\n",
            "Fetching posts for LOW...\n",
            "Fetched 15 posts for LOW\n",
            "\n",
            "Fetching posts for RTX...\n",
            "Fetched 15 posts for RTX\n",
            "Saved 300 posts to reddit_stock_posts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvOetzJym7s-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}