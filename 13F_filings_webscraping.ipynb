{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5260a415-d56a-461e-bb00-9959ae13b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25faa4e6-93a5-4403-bac7-aa6c48489cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_institutions = {\n",
    "    \"0001067983\": \"Berkshire Hathaway\",\n",
    "    #\"0001166559\": \"Renaissance Technologies\"\n",
    "    #\"0001037389\": \"Citadel Advisors\",\n",
    "    #\"0001081060\": \"BlackRock Inc.\",\n",
    "    #\"0001103804\": \"Bridgewater Associates\",\n",
    "    #\"0000922971\": \"Two Sigma Investments\",\n",
    "    #\"0001079114\": \"Millennium Management\",\n",
    "    #\"0000912057\": \"Vanguard Group\",\n",
    "    #\"0000316927\": \"FMR LLC\",\n",
    "    #\"0000354204\": \"State Street Corp\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190ed4f-b462-45a4-8361-9b0de6eb3920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca36e746-7009-4055-a0f6-04502375b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_filing_metadata(cik, institution_name):\n",
    "    url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=13F-HR&owner=exclude&count=40&output=atom\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"DataScienceInternshipBot/1.0 (contact: chandanarchutha.n@gmail.com)\",\n",
    "        \"Accept\": \"application/xml\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://www.sec.gov/\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    entries = soup.find_all(\"entry\")\n",
    "\n",
    "    filing_links = []\n",
    "    filed_dates = []\n",
    "    institution_names = []\n",
    "\n",
    "    for entry in entries:\n",
    "        link = entry.find(\"link\")[\"href\"].strip()\n",
    "        date = entry.find(\"updated\").text.strip()\n",
    "        filing_links.append(link)\n",
    "        filed_dates.append(date)\n",
    "        institution_names.append(institution_name)\n",
    "\n",
    "    return filing_links, filed_dates, institution_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f9f7b0-c6fa-4811-8ed1-cbbd89f52651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract XML info table link from index page\n",
    "def extract_info_table_xml(index_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"DataScienceInternshipBot/1.0 (contact: chandanarchutha.n@gmail.com)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(index_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        table = soup.find(\"table\", class_=\"tableFile\")\n",
    "        if table:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                cols = row.find_all(\"td\")\n",
    "                if len(cols) >= 3:\n",
    "                    a_tag = cols[2].find(\"a\")\n",
    "                    doc_type = cols[3].text.strip().lower()\n",
    "                    if a_tag:\n",
    "                        href = a_tag.get(\"href\", \"\")\n",
    "                        if href.endswith(\".xml\") and \"form13f\" in href.lower() and doc_type == \"information table\":\n",
    "                            return \"https://www.sec.gov\" + href\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error parsing {index_url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4c84f6-260a-4988-b3bb-735364151d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_13f_holdings_from_html(xml_url, filed_date, institution_name):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"DataScienceInternshipBot/1.0 (contact: chandanarchutha.n@gmail.com)\",\n",
    "        \"Accept\": \"application/xml\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(xml_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if not tables:\n",
    "        return []\n",
    "\n",
    "    data_table = tables[-1]\n",
    "    rows = data_table.find_all(\"tr\")\n",
    "    holdings = []\n",
    "\n",
    "    for row in rows[4:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) >= 13:\n",
    "            issuer = cols[0].text.strip()\n",
    "            class_title = cols[1].text.strip()\n",
    "            cusip = cols[2].text.strip()\n",
    "            value = cols[4].text.strip()\n",
    "            shares = cols[5].text.strip()\n",
    "            discretion = cols[8].text.strip()\n",
    "            voting_sole = cols[10].text.strip()\n",
    "            voting_shared = cols[11].text.strip()\n",
    "            voting_none = cols[12].text.strip()\n",
    "\n",
    "            holdings.append([\n",
    "                issuer, class_title, cusip, value, shares, discretion,\n",
    "                voting_sole, voting_shared, voting_none, filed_date, xml_url, institution_name\n",
    "            ])\n",
    "    return holdings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd816599-ea80-4fe7-809b-05ddcac05e59",
   "metadata": {},
   "source": [
    "all_holdings = []\n",
    "\n",
    "for cik, institution_name in top_institutions.items():\n",
    "    filing_links, filed_dates, institution_names = fetch_filing_metadata(cik, institution_name)\n",
    "    xml_links = [extract_info_table_xml(link) for link in filing_links]\n",
    "    sample_filings = list(zip(xml_links, filed_dates, institution_names))\n",
    "\n",
    "    for xml_url, filed_date, institution_name in sample_filings:\n",
    "        if xml_url:\n",
    "            try:\n",
    "                extracted = extract_13f_holdings_from_html(xml_url, filed_date, institution_name)\n",
    "                all_holdings.extend(extracted)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {xml_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8311aa-c457-4cb0-a757-6eac298e4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/40 filings for Berkshire Hathaway...\n",
      "Processed 6/40 filings for Berkshire Hathaway...\n",
      "Processed 11/40 filings for Berkshire Hathaway...\n",
      "Processed 16/40 filings for Berkshire Hathaway...\n",
      "Processed 21/40 filings for Berkshire Hathaway...\n",
      "Processed 26/40 filings for Berkshire Hathaway...\n",
      "Processed 31/40 filings for Berkshire Hathaway...\n",
      "Processed 36/40 filings for Berkshire Hathaway...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Final version of scraping loop with improvements\n",
    "all_holdings = []\n",
    "failed_urls = []\n",
    "\n",
    "for cik, institution_name in top_institutions.items():\n",
    "    filing_links, filed_dates, institution_names = fetch_filing_metadata(cik, institution_name)\n",
    "    xml_links = [extract_info_table_xml(link) for link in filing_links]\n",
    "    sample_filings = list(zip(xml_links, filed_dates, institution_names))\n",
    "    for i, (xml_url, filed_date, institution_name) in enumerate(sample_filings):\n",
    "        if xml_url:\n",
    "            try:\n",
    "                extracted = extract_13f_holdings_from_html(xml_url, filed_date, institution_name)\n",
    "                all_holdings.extend(extracted)\n",
    "                if i % 5 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(sample_filings)} filings for {institution_name}...\")\n",
    "                time.sleep(0.5)  # polite pause between requests to avoid SEC throttling\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {xml_url}: {e}\")\n",
    "                failed_urls.append((xml_url, institution_name))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if failed_urls:\n",
    "    print(f\"\\n❌ {len(failed_urls)} filings failed to parse. You can retry them later if needed.\")\n",
    "    for url, name in failed_urls:\n",
    "        print(f\"- {name}: {url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28362270-604a-4bd5-8ebc-a0fe26d501bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_holdings, columns=[\n",
    "    \"Issuer\", \"Class\", \"CUSIP\", \"Value (x$1000)\", \"Shares\", \"Discretion\",\n",
    "    \"Voting - Sole\", \"Voting - Shared\", \"Voting - None\", \"Filed Date\", \"Source URL\", \"Institution\"\n",
    "])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ea7b49-09cd-4d76-a6d6-26bc03b82795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1530 entries, 0 to 1529\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Issuer           1530 non-null   object\n",
      " 1   Class            1530 non-null   object\n",
      " 2   CUSIP            1530 non-null   object\n",
      " 3   Value (x$1000)   1530 non-null   object\n",
      " 4   Shares           1530 non-null   object\n",
      " 5   Discretion       1530 non-null   object\n",
      " 6   Voting - Sole    1530 non-null   object\n",
      " 7   Voting - Shared  1530 non-null   object\n",
      " 8   Voting - None    1530 non-null   object\n",
      " 9   Filed Date       1530 non-null   object\n",
      " 10  Source URL       1530 non-null   object\n",
      " 11  Institution      1530 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 143.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
